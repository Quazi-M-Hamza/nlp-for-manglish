{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MA_YT001</td>\n",
       "      <td>Thaankal enthaan cheyyarullath?😛</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MA_YT002</td>\n",
       "      <td>Ee theetam WCC feminichigalude news aarkk vena...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MA_YT003</td>\n",
       "      <td>fukru nem tiktok oolakale vilich charcha nadat...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MA_YT004</td>\n",
       "      <td>Aashiq abu produce cheytharunnel ee problems u...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MA_YT005</td>\n",
       "      <td>Pennungal oru team aayal ath moonjum ennu epoo...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                             Tweets Labels\n",
       "0  MA_YT001                   Thaankal enthaan cheyyarullath?😛    NOT\n",
       "1  MA_YT002  Ee theetam WCC feminichigalude news aarkk vena...    OFF\n",
       "2  MA_YT003  fukru nem tiktok oolakale vilich charcha nadat...    OFF\n",
       "3  MA_YT004  Aashiq abu produce cheytharunnel ee problems u...    NOT\n",
       "4  MA_YT005  Pennungal oru team aayal ath moonjum ennu epoo...    OFF"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_excel(f'../../../code-mixed-enma2/code-mixed-enma-2/hasoc_task_2/Malayalam_offensive_data_Training-YT.xlsx')\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MA_YT5000</td>\n",
       "      <td>Chenkol vendath thanne aayirunnu....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MA_YT5001</td>\n",
       "      <td>Sundardasinte bhakshnam vakkukal ano?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MA_YT5002</td>\n",
       "      <td>Akasha dooth oru copy adi movie anu 'Who will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MA_YT5003</td>\n",
       "      <td>Purath onnum pondade... oru pennum payyanum on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MA_YT5004</td>\n",
       "      <td>Avasanam Fahad oru Oscar medikkumbazhum lalett...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                                                  1\n",
       "0  MA_YT5000               Chenkol vendath thanne aayirunnu....\n",
       "1  MA_YT5001              Sundardasinte bhakshnam vakkukal ano?\n",
       "2  MA_YT5002  Akasha dooth oru copy adi movie anu 'Who will ...\n",
       "3  MA_YT5003  Purath onnum pondade... oru pennum payyanum on...\n",
       "4  MA_YT5004  Avasanam Fahad oru Oscar medikkumbazhum lalett..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(f'../../../code-mixed-enma2/code-mixed-enma-2/hasoc_task_2/malayalam_hasoc_tanglish_test_without_labels.tsv', sep='\\t', header=None)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "latin_letters= {}\n",
    "\n",
    "def is_latin(uchr):\n",
    "    try: return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "         return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "    return all(is_latin(uchr)\n",
    "           for uchr in unistr\n",
    "           if uchr.isalpha()) # isalpha suggested by John Machin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 Evl Commited anenðŸ¥º lokath ethilum gethikettavn vere arum kanilaðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ OFF\n",
      "\n",
      "\n",
      "\n",
      "311 Veera paniyille sahooo... Tym waisted..... ðŸ¤ª enthuvaadai eth NOT\n",
      "\n",
      "\n",
      "\n",
      "415 Valla varka panikum poykoode..ijjathi itetheyoke interview cheyanðŸ¥ºðŸ¥º OFF\n",
      "\n",
      "\n",
      "\n",
      "469 Orange thopURL satyam eth enthonnu ðŸ˜‚ðŸ¤ª NOT\n",
      "\n",
      "\n",
      "\n",
      "492 Ennalum keralathilum engane kuru pottunnavar undalloo ennu oorkkumbozhaðŸ˜ªðŸ˜ª NOT\n",
      "\n",
      "\n",
      "\n",
      "556 Ingalu elladathum undallo â˜ºï¸ðŸ˜ƒ NOT\n",
      "\n",
      "\n",
      "\n",
      "628 Paavam aa chknte kili poyeenn thonnan history kettitt ðŸ˜‚ðŸ˜‚ After *Le boy: pullu parayandarnnðŸ˜ª NOT\n",
      "\n",
      "\n",
      "\n",
      "649 Athrakkk Artificial ayindðŸ¤ªðŸ¤ªðŸ¤ª NOT\n",
      "\n",
      "\n",
      "\n",
      "670 Aaadyam aa chekkanitta kodukkendath.. Aa monjathiye propose cheyyaan poyeen ðŸ˜ªðŸ˜ŒðŸ˜‚ OFF\n",
      "\n",
      "\n",
      "\n",
      "688 Ath angine kure kanaa konaa teams...ðŸ˜ðŸ¤ªðŸ˜œ NOT\n",
      "\n",
      "\n",
      "\n",
      "977 Cheriya kaliyallaâ˜ºprabudharaaya janamðŸ˜¢ðŸ˜¢ NOT\n",
      "\n",
      "\n",
      "\n",
      "979 Ethilipom arayan serikum vellam kudiURL â˜ºðŸ˜‚ NOT\n",
      "\n",
      "\n",
      "\n",
      "1004 Next CM URL vijayan Ni okke evda kidanond korachukondee erikum... Karanam korakkum poorikall kadikillaðŸ¤£ðŸ¤£ðŸ˜µ OFF\n",
      "\n",
      "\n",
      "\n",
      "1061 Pokaan para nummaluntt koodey athonnum kaaryamakantta ðŸ’ªðŸ’ªðŸ’ª NOT\n",
      "\n",
      "\n",
      "\n",
      "1266 24:57 kelkkuka..... Sangethika Prasnam kopppa ðŸ˜¤ðŸ˜¤ðŸ˜¤ðŸ˜¤ðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ¤’ðŸ˜ ðŸ˜ ðŸ˜ ðŸ˜ ðŸ˜ ðŸ˜ ðŸ˜ ðŸ˜ kelakkan Podooo..... Oodakkuzhale OFF\n",
      "\n",
      "\n",
      "\n",
      "1340 @USER mohamu yudham cheiyan nammude nadineyum nammal ellapereyum samrakshikkan Ramanum.. Antonyum... Hameedum... Indiayude borderil indu ðŸ’ª NOT\n",
      "\n",
      "\n",
      "\n",
      "1789 Aa kuttykalude kaaryam orkkumbol sankadam thonnunnu....ðŸ˜ª NOT\n",
      "\n",
      "\n",
      "\n",
      "1964 athe njan oru kaariyam paranjotte aarum thettidharikaruth.... avark eppolum idanum ooranum bhudhimuttu aayath kondayirikkum avar idathathhhðŸ™ˆðŸ™‰ðŸ™Š OFF\n",
      "\n",
      "\n",
      "\n",
      "1966 @USER Dud ennalum...onnu paraðŸ˜name parayunnath kond nthelum prblm indo indenkil vendatto verthe chothichunne ullu chettayi â˜º NOT\n",
      "\n",
      "\n",
      "\n",
      "1975 E commt kanda udane head setnu thappunna njnðŸ¤ª NOT\n",
      "\n",
      "\n",
      "\n",
      "2002 Bro oola cinimakal review chaithu koode ðŸ’ªðŸ’ªðŸ’ªðŸ’ªðŸ’ªðŸ’ª polikum NOT\n",
      "\n",
      "\n",
      "\n",
      "2008 Ithoke anu namude natile virus ðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆðŸ˜ˆ NOT\n",
      "\n",
      "\n",
      "\n",
      "2017 @USER fathima uluppp anna vaakyam polum aduth povaatha orannam! Naked womenðŸ™ˆðŸ™ˆðŸ™ˆðŸ™ˆðŸ™ˆ OFF\n",
      "\n",
      "\n",
      "\n",
      "2097 @USER GEORGE K ðŸ¦µ legins ittaal ippam pennukuttikaluday shedi kannaam makkaley OFF\n",
      "\n",
      "\n",
      "\n",
      "2104 @USER nammade ammavande mol aanenn thonunnu...... avarkk thuniyillathe aanu nadakkan ishttamenkil public aayitt angane nadakatte..... athinulla swadhanthryam avarkundallo..... athu pole baaki ullavarkk abiprayam parayanum... ðŸ¤ª OFF\n",
      "\n",
      "\n",
      "\n",
      "2214 Ente chennal onn support chynaaâ˜º NOT\n",
      "\n",
      "\n",
      "\n",
      "2216 Ith cheguthaante kottayaaanðŸ˜ˆ NOT\n",
      "\n",
      "\n",
      "\n",
      "3196 Sir, ningal oru mass Anu .. Point to point dialogue superbðŸ’ªðŸ»ðŸ‘ðŸ¼ðŸ‘ðŸ¼ NOT\n",
      "\n",
      "\n",
      "\n",
      "3627 Ninta achan atho mamanta koode pona njan kanduâ˜º NOT\n",
      "\n",
      "\n",
      "\n",
      "3713 Ithippo pretha mayam aanallo Uppum mulakum kazhinj de ippo ithðŸ¤ªðŸ‘» NOT\n",
      "\n",
      "\n",
      "\n",
      "3807 Appo njn mathram allah lee nte umma parym lookhathh nee maathrm aahn kaanchodkkam ummachiinte munninn thanne njn ornghmðŸ˜µðŸ˜ðŸ˜ NOT\n",
      "\n",
      "\n",
      "\n",
      "3903 @USER Sunny ðŸ‘ðŸ‘ nice song ithrayum nalla reethiyil paadumelleðŸ˜µ NOT\n",
      "\n",
      "\n",
      "\n",
      "0.991997999499875\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index, row in df_train.iterrows():\n",
    "    if not only_roman_chars(row['Tweets']):\n",
    "        print(index, row['Tweets'], row['Labels'])\n",
    "        print('\\n\\n')\n",
    "        count +=1\n",
    "print(1 - count/len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index, row in df_test.iterrows():\n",
    "    if not only_roman_chars(row[1]):\n",
    "        print(index, row[1])\n",
    "        print('\\n\\n')\n",
    "        count +=1\n",
    "print(1 - count/len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Class distribution in training set\n",
    "D = Counter(df_train['Labels'])\n",
    "plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "plt.bar(range(len(D)), list(D.values()), align='center')\n",
    "plt.xticks(range(len(D)), list(D.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min no. of sentences:  1952\n",
      "Max no. of sentences:  2047\n",
      "Avg no. of sentences:  1999.5\n",
      "Median of sentences:  1999.5\n"
     ]
    }
   ],
   "source": [
    "# Min, Max, Avg no of sentences per class in Training set\n",
    "print('Min no. of sentences: ', min(D.values()))\n",
    "print('Max no. of sentences: ', max(D.values()))\n",
    "print('Avg no. of sentences: ', mean(D.values()))\n",
    "print('Median of sentences: ', median(D.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s: str):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min no. of tokens:  1\n",
      "Max no. of tokens:  62\n",
      "Avg no. of tokens:  9.99349837459365\n",
      "Median of no. of tokens:  8\n"
     ]
    }
   ],
   "source": [
    "# variation in length of sentences in train set\n",
    "len_of_tokens = []\n",
    "for index, row in df_train.iterrows():\n",
    "    tokens = tokenize((row['Tweets']).lower())\n",
    "    len_of_tokens.append(len(tokens))\n",
    "print('Min no. of tokens: ', min(len_of_tokens))\n",
    "print('Max no. of tokens: ', max(len_of_tokens))\n",
    "print('Avg no. of tokens: ', mean(len_of_tokens))\n",
    "print('Median of no. of tokens: ', median(len_of_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min no. of tokens:  1\n",
      "Max no. of tokens:  526\n",
      "Avg no. of tokens:  10.47844374342797\n",
      "Median of no. of tokens:  9\n"
     ]
    }
   ],
   "source": [
    "# variation in length of sentences in test set\n",
    "len_of_tokens = []\n",
    "for index, row in df_test.iterrows():\n",
    "    tokens = tokenize((row[1]).lower())\n",
    "    len_of_tokens.append(len(tokens))\n",
    "print('Min no. of tokens: ', min(len_of_tokens))\n",
    "print('Max no. of tokens: ', max(len_of_tokens))\n",
    "print('Avg no. of tokens: ', mean(len_of_tokens))\n",
    "print('Median of no. of tokens: ', median(len_of_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
